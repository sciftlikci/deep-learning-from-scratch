{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_with_numpy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code for 2-layer NN model"
      ],
      "metadata": {
        "id": "RTLBmtU5iEei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "np.random.seed(0)\n",
        "\n",
        "X_train_path = None\n",
        "y_train_path = None\n",
        "X_test_path = None\n",
        "y_test_path = None\n",
        "\n",
        "train_df = pd.DataFrame(np.load(X_train_path), columns=[\"feature_1\", \"feature_2\"])\n",
        "train_df[\"target\"] = np.load(y_train_path)\n",
        "test_df = pd.DataFrame(np.load(X_test_path), columns=[\"feature_1\", \"feature_2\"])\n",
        "test_df[\"target\"] = np.load(y_test_path)\n",
        "\n",
        "class Sigmoid:\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_prime(self, x):\n",
        "        s = self.sigmoid(x)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.sigmoid(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate=None):\n",
        "        return np.multiply(output_gradient, self.sigmoid(self.input))\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_prime(x):\n",
        "        return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.relu(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate=None):\n",
        "        return np.multiply(output_gradient, self.relu_prime(self.input))\n",
        "\n",
        "\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # He normal initialization as advised by He et al., 2015\n",
        "        self.weights = np.random.normal(0, np.sqrt(2 / input_size), \n",
        "                                        size = (output_size, input_size))\n",
        "        self.bias = np.zeros((output_size, 1))\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(self.weights, self.input) + self.bias\n",
        "\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        return input_gradient    \n",
        "\n",
        "\n",
        "# loss function\n",
        "def binary_cross_entropy(y_true, y_pred, epsilon = 1e-8):\n",
        "    # clipping for stable log operation\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) \n",
        "    return (-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)).item()\n",
        "\n",
        "\n",
        "def binary_cross_entropy_prime(y_true, y_pred):\n",
        "    return (1 - y_true) / (1 - y_pred) - (y_true / y_pred)\n",
        "\n",
        "\n",
        "def predict(trained_network, x, thresh = 0.5):\n",
        "    \"\"\"helper for inference with a trained neural network model.\"\"\"\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        out = None\n",
        "\n",
        "        if isinstance(x, pd.DataFrame):\n",
        "            out = x.loc[i, [\"feature_1\", \"feature_2\"]].values.reshape(-1, 1)\n",
        "        else:\n",
        "            out = x[i].reshape(-1, 1)\n",
        "        \n",
        "        for layer in trained_network:\n",
        "            out = layer.forward(out)\n",
        "\n",
        "        preds.append(1 if out.item() >= thresh else 0)\n",
        "\n",
        "    return np.array(preds)\n"
      ],
      "metadata": {
        "id": "aQgcXwEgiNOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & evaluate the model"
      ],
      "metadata": {
        "id": "FYoE85SvQ3hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary hyperparameters\n",
        "N_EPOCHS = 450\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_SIZE = 100\n",
        "INPUT_SIZE = len(train_df.columns) - 1\n",
        "OUTPUT_SIZE = 1\n",
        "\n",
        "# model architecture\n",
        "network = [\n",
        "              Dense(INPUT_SIZE, HIDDEN_SIZE),\n",
        "              ReLU(),\n",
        "              Dense(HIDDEN_SIZE, OUTPUT_SIZE),\n",
        "              Sigmoid()\n",
        "          ]\n",
        " \n",
        "# training loop\n",
        "for e in range(N_EPOCHS):\n",
        "    loss = 0\n",
        "\n",
        "    for index, row in train_df.iterrows():\n",
        "        x = np.array(row.values[:-1]).reshape(-1, 1)\n",
        "        y = row.values[-1]\n",
        "\n",
        "        for layer in network:\n",
        "            # forward prop\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        # loss calculation\n",
        "        loss += binary_cross_entropy(y, x)\n",
        "\n",
        "        # backward prop\n",
        "        grad = binary_cross_entropy_prime(y, x)\n",
        "\n",
        "        for layer in reversed(network):\n",
        "            grad = layer.backward(grad, LEARNING_RATE)\n",
        "\n",
        "    # averaging\n",
        "    loss /= len(train_df)\n",
        "\n",
        "    # logging\n",
        "    if e % 50 == 0:\n",
        "        print(\"Epoch: {}, Average BCE Loss = {:.4f}\".format(e, loss))\n",
        "\n",
        "\n",
        "test_preds = predict(network, test_df[list(test_df.columns)[:-1]].values) \n",
        "print(\"Test Accuracy: {}\".format(np.mean((np.equal(test_df[\"target\"].values, test_preds) * 1))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu26GP9Knrux",
        "outputId": "8ff27e22-94a2-41ad-f517-0e91169a2af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average BCE Loss = 0.6933\n",
            "Epoch: 50, Average BCE Loss = 0.5038\n",
            "Epoch: 100, Average BCE Loss = 0.4238\n",
            "Epoch: 150, Average BCE Loss = 0.3623\n",
            "Epoch: 200, Average BCE Loss = 0.3137\n",
            "Epoch: 250, Average BCE Loss = 0.2727\n",
            "Epoch: 300, Average BCE Loss = 0.2411\n",
            "Epoch: 350, Average BCE Loss = 0.2186\n",
            "Epoch: 400, Average BCE Loss = 0.2037\n",
            "Test Accuracy: 0.875\n"
          ]
        }
      ]
    }
  ]
}